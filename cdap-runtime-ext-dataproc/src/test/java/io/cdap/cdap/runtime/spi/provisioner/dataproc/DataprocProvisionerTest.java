/*
 * Copyright Â© 2018-2020 Cask Data, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package io.cdap.cdap.runtime.spi.provisioner.dataproc;

import com.google.cloud.dataproc.v1.ClusterOperationMetadata;
import com.google.cloud.dataproc.v1.ClusterStatus.State;
import com.google.common.collect.ImmutableMap;
import io.cdap.cdap.api.exception.ErrorCategory;
import io.cdap.cdap.api.exception.ErrorCategory.ErrorCategoryEnum;
import io.cdap.cdap.runtime.spi.MockVersionInfo;
import io.cdap.cdap.runtime.spi.ProgramRunInfo;
import io.cdap.cdap.runtime.spi.SparkCompat;
import io.cdap.cdap.runtime.spi.common.DataprocImageVersion;
import io.cdap.cdap.runtime.spi.provisioner.Cluster;
import io.cdap.cdap.runtime.spi.provisioner.ClusterStatus;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Optional;
import java.util.UUID;
import java.util.function.Predicate;
import java.util.stream.Stream;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.mockito.ArgumentCaptor;
import org.mockito.Captor;
import org.mockito.Mock;
import org.mockito.Mockito;
import org.mockito.runners.MockitoJUnitRunner;

/**
 * Tests for Dataproc provisioner
 */
@RunWith(MockitoJUnitRunner.class)
public class DataprocProvisionerTest {
  private static final String RESOURCE_MAX_PERCENT_KEY =
    "capacity-scheduler:yarn.scheduler.capacity.maximum-am-resource-percent";
  private static final String RESOURCE_MAX_PERCENT_VAL = "0.5";
  private static final String CLUSTER_META_DATA =
      "metadata-key1|metadata-val1;metadata-key2|metadata-val2";
  private static final String SERVICE_ACCOUNT = "service-account-1";
  private static final String tokTOKEN_ENDPOINTnEndpoint = "end-point1";

  @Mock
  private DataprocClient dataprocClient;
  @Mock
  private Cluster cluster;
  @Mock
  private Cluster cluster2;

  private DataprocProvisioner provisioner;
  @Captor
  private ArgumentCaptor<Map<String, String>> addedLabelsCaptor;

  MockProvisionerContext context = new MockProvisionerContext();

  @Before
  public void init() {
    provisioner = new DataprocProvisioner((conf, requireSsh, errorCategory) -> dataprocClient);
    MockProvisionerSystemContext provisionerSystemContext = new MockProvisionerSystemContext();

    // default system properties defined by DataprocProvisioner
    provisionerSystemContext.addProperty(DataprocConf.NETWORK, "old-network");
    provisionerSystemContext.addProperty(DataprocConf.STACKDRIVER_LOGGING_ENABLED, "true");
    provisionerSystemContext
      .addProperty(DataprocConf.CLUSTER_META_DATA, CLUSTER_META_DATA);
    provisionerSystemContext.addProperty(DataprocConf.SERVICE_ACCOUNT, SERVICE_ACCOUNT);
    provisionerSystemContext.addProperty(DataprocConf.TOKEN_ENDPOINT_KEY, tokTOKEN_ENDPOINTnEndpoint);

    // default system properties defined by AbstractDataprocProvisioner
    provisionerSystemContext
        .addProperty(RESOURCE_MAX_PERCENT_KEY, RESOURCE_MAX_PERCENT_VAL);
    provisionerSystemContext
        .addProperty(DataprocConf.RUNTIME_JOB_MANAGER, "job_manager");
    provisionerSystemContext
        .addProperty("labels", "system-prop=value,user=system");
    // non-default system properties
    provisionerSystemContext.addProperty("non-system-default-key", "any-value");
    provisionerSystemContext.setCDAPVersion("6.4");

    provisioner.initialize(provisionerSystemContext);
  }

  @Test
  public void testRunKey() throws Exception {
    // test basic
    ProgramRunInfo programRunInfo = new ProgramRunInfo.Builder()
      .setNamespace("ns")
      .setApplication("app")
      .setVersion("1.0")
      .setProgramType("workflow")
      .setProgram("program")
      .setRun(UUID.randomUUID().toString())
      .build();
    Assert.assertEquals("cdap-app-" + programRunInfo.getRun(),
        new DataprocProvisioner().getRunKey(
            new MockProvisionerContext(programRunInfo)));

    // test lowercasing, stripping of invalid characters, and truncation
    programRunInfo = new ProgramRunInfo.Builder()
      .setNamespace("ns")
      .setApplication("My@Appl!cation")
      .setVersion("1.0")
      .setProgramType("workflow")
      .setProgram("program")
      .setRun(UUID.randomUUID().toString())
      .build();
    Assert.assertEquals("cdap-myapplcat-" + programRunInfo.getRun(),
        new DataprocProvisioner().getRunKey(
            new MockProvisionerContext(programRunInfo)));
  }

  @Test
  public void testDataprocConf() {
    Map<String, String> props = new HashMap<>();
    String scopes = String.format("%s,https://www.googleapis.com/auth/drive",
        DataprocConf.CLOUD_PLATFORM_SCOPE);

    props.put(DataprocConf.PROJECT_ID_KEY, "pid");
    props.put("accountKey", "key");
    props.put("region", "region1");
    props.put("zone", "region1-a");
    props.put("network", "network");
    props.put("spark:spark.reducer.maxSizeInFlight", "100");
    props.put("hadoop-env:MAPREDUCE_CLASSPATH", "xyz");
    props.put("dataproc:am.primary_only", "true");
    props.put("clusterMetaData",
        "metadata-key1|metadata-val1;metadata-key2|metadata-val2");
    props.put("clusterLabels", "label-key1|label-val1;label-key2|label-val2");
    props.put("token.endpoint", "point1");
    props.put("secureBootEnabled", "false");
    props.put("vTpmEnabled", "true");
    props.put("integrityMonitoringEnabled", "true");
    props.put("idleTTL", "20");
    props.put("clusterReuseRetryDelayMs", "20");
    props.put("clusterReuseRetryMaxMs", "200");
    props.put(DataprocConf.SCOPES, scopes);

    DataprocConf conf = DataprocConf.create(props);

    Assert.assertEquals("pid", conf.getProjectId());
    Assert.assertEquals("region1", conf.getRegion());
    Assert.assertEquals("region1-a", conf.getZone());
    Assert.assertEquals("point1", conf.getTokenEndpoint());
    Assert.assertEquals(20, conf.getIdleTtlMinutes());
    Map<String, String> clusterMetaData = conf.getClusterMetaData();
    Assert.assertEquals("metadata-val1", clusterMetaData.get("metadata-key1"));
    Assert.assertEquals("metadata-val2", clusterMetaData.get("metadata-key2"));

    Map<String, String> clusterLabels = conf.getClusterLabels();
    Assert.assertEquals("label-val1", clusterLabels.get("label-key1"));
    Assert.assertEquals("label-val2", clusterLabels.get("label-key2"));

    Map<String, String> dataprocProps = conf.getClusterProperties();
    Assert.assertEquals(3, dataprocProps.size());

    Assert.assertEquals("100",
        dataprocProps.get("spark:spark.reducer.maxSizeInFlight"));
    Assert.assertEquals("xyz",
        dataprocProps.get("hadoop-env:MAPREDUCE_CLASSPATH"));
    Assert.assertEquals("true", dataprocProps.get("dataproc:am.primary_only"));

    Assert.assertFalse(conf.isSecureBootEnabled());
    Assert.assertTrue(conf.isvTpmEnabled());
    Assert.assertTrue(conf.isIntegrityMonitoringEnabled());

    Assert.assertEquals(20, conf.getClusterReuseRetryDelayMs());
    Assert.assertEquals(200, conf.getClusterReuseRetryMaxMs());
    Assert.assertEquals(scopes, String.join(",", conf.getScopes()));
  }

  @Test
  public void testDataprocConfDefaultIdleTtl() {
    Map<String, String> props = new HashMap<>();
    props.put(DataprocConf.PROJECT_ID_KEY, "pid");
    props.put("accountKey", "key");
    props.put("region", "region1");
    DataprocConf conf = DataprocConf.create(props);
    Assert.assertEquals(30, conf.getIdleTtlMinutes());
  }

  @Test
  public void testAutoZone() {
    Map<String, String> props = new HashMap<>();
    props.put(DataprocConf.PROJECT_ID_KEY, "pid");
    props.put("accountKey", "key");
    props.put("region", "region1");
    props.put("zone", "auto-detect");
    props.put("network", "network");

    DataprocConf conf = DataprocConf.create(props);
    Assert.assertNull(conf.getZone());
  }

  @Test(expected = IllegalArgumentException.class)
  public void testInvalidZoneCheck() {
    Map<String, String> props = new HashMap<>();
    props.put(DataprocConf.PROJECT_ID_KEY, "pid");
    props.put("accountKey", "key");
    props.put("region", "region1");
    props.put("zone", "region2-a");
    props.put("network", "network");

    DataprocConf.create(props);
  }

  @Test
  public void testCreateContextProperties() {
    final String network = "test-network";
    context.addProperty(DataprocConf.NETWORK, network);

    Map<String, String> properties = provisioner.createContextProperties(
        context);

    Assert.assertEquals(network, properties.get(DataprocConf.NETWORK));
    Assert.assertEquals("true",
        properties.get(DataprocConf.STACKDRIVER_LOGGING_ENABLED));
    Assert.assertEquals(RESOURCE_MAX_PERCENT_VAL,
        properties.get(RESOURCE_MAX_PERCENT_KEY));
    Assert.assertEquals(CLUSTER_META_DATA,
        properties.get(DataprocConf.CLUSTER_META_DATA));
    Assert.assertEquals(SERVICE_ACCOUNT,
        properties.get(DataprocConf.SERVICE_ACCOUNT));
    Assert.assertEquals(tokTOKEN_ENDPOINTnEndpoint,
        properties.get(DataprocConf.TOKEN_ENDPOINT_KEY));
    Assert.assertEquals("job_manager",
        properties.get(DataprocConf.RUNTIME_JOB_MANAGER));
    Assert.assertNull(properties.get("non-system-default-key"));
  }

  @Test
  public void testCustomImageUri() {
    Map<String, String> props = new HashMap<>();
    String customUri = "https://www.googleapis.com/compute/v1/projects/p1/global/images/testimage";
    props.put(DataprocConf.CUSTOM_IMAGE_URI, customUri);
    props.put("accountKey", "key");
    props.put("projectId", "my project");
    props.put("zone", "region1-a");

    DataprocConf conf = DataprocConf.create(props);
    Assert.assertEquals(customUri, conf.getCustomImageUri());
  }

  @Test
  public void testGetImageVersion() {
    DataprocConf defaultConf = DataprocConf.create(ImmutableMap.of(
      "accountKey", "key",
      "projectId", "my project",
      "zone", "region1-a"
    ));
    DataprocConf explicitVersionConf = DataprocConf.create(ImmutableMap.of(
      "accountKey", "key",
      "projectId", "my project",
      "zone", "region1-a",
      DataprocConf.IMAGE_VERSION, "explicit"
    ));

    context.setSparkCompat(SparkCompat.SPARK3_2_12);
    Assert.assertEquals("2.1", provisioner.getImageVersion(context, defaultConf));
    Assert.assertEquals("explicit", provisioner.getImageVersion(context, explicitVersionConf));

    context.setAppCDAPVersionInfo(new MockVersionInfo("6.5.0"));
    Assert.assertEquals("2.1", provisioner.getImageVersion(context, defaultConf));
    Assert.assertEquals("explicit", provisioner.getImageVersion(context, explicitVersionConf));

    context.setAppCDAPVersionInfo(new MockVersionInfo("6.4.0"));
    Assert.assertEquals("2.1", provisioner.getImageVersion(context, defaultConf));
    Assert.assertEquals("explicit", provisioner.getImageVersion(context, explicitVersionConf));

    //Doublecheck we still get 2.1 for Spark 3 even with CDAP 6.4
    context.setSparkCompat(SparkCompat.SPARK3_2_12);
    Assert.assertEquals("2.1", provisioner.getImageVersion(context, defaultConf));

  }

  @Test
  public void testClusterCreateNoReuse() throws Exception {
    context.addProperty("accountKey", "testKey");
    context.addProperty(DataprocConf.PROJECT_ID_KEY, "testProject");
    context.addProperty("region", "testRegion");
    context.addProperty("idleTTL", "5");
    context.addProperty(DataprocConf.SKIP_DELETE, "true");
    context.setProfileName("testProfile");
    ProgramRunInfo programRunInfo = new ProgramRunInfo.Builder()
      .setNamespace("ns")
      .setApplication("app")
      .setVersion("1.0")
      .setProgramType("workflow")
      .setProgram("program")
      .setRun("runId")
      .build();
    context.setProgramRunInfo(programRunInfo);
    context.setSparkCompat(SparkCompat.SPARK3_2_12);
    context.addProperty(DataprocConf.CLUSTER_REUSE_ENABLED, "false");
    context.setErrorCategory(new ErrorCategory(ErrorCategoryEnum.PROVISIONING));

    Mockito.when(dataprocClient.getCluster("cdap-app-runId"))
        .thenReturn(Optional.empty());
    Mockito.when(dataprocClient.createCluster(Mockito.eq("cdap-app-runId"),
            Mockito.eq("2.1"), addedLabelsCaptor.capture(), Mockito.eq(false),
            Mockito.any()))
      .thenReturn(ClusterOperationMetadata.getDefaultInstance());
    Cluster expectedCluster = new Cluster(
      "cdap-app-runId", ClusterStatus.CREATING, Collections.emptyList(), Collections.emptyMap());
    Assert.assertEquals(expectedCluster, provisioner.createCluster(context));
    HashMap<String, String> expectedLables = new HashMap<>();
    expectedLables.put("cdap-version", "6_4");
    expectedLables.put("system-prop", "value");
    expectedLables.put("user", "system");
    Assert.assertEquals(expectedLables,
        addedLabelsCaptor.getValue());
  }

  @Test
  public void testClusterReuseOnCreate() throws Exception {
    context.addProperty("accountKey", "testKey");
    context.addProperty(DataprocConf.PROJECT_ID_KEY, "testProject");
    context.addProperty("region", "testRegion");
    context.addProperty("idleTTL", "5");
    context.addProperty(DataprocConf.SKIP_DELETE, "true");
    context.setProfileName("testProfile");
    context.addProperty(DataprocConf.CLUSTER_REUSE_RETRY_DELAY_MS, "0");
    ProgramRunInfo programRunInfo = new ProgramRunInfo.Builder()
      .setNamespace("ns")
      .setApplication("app")
      .setVersion("1.0")
      .setProgramType("workflow")
      .setProgram("program")
      .setRun("runId")
      .build();
    context.setProgramRunInfo(programRunInfo);
    context.setErrorCategory(new ErrorCategory(ErrorCategoryEnum.PROVISIONING));

    //A. Check with existing client, probably after a retry
    Mockito.when(dataprocClient.getClusters(
            Collections.singletonMap(AbstractDataprocProvisioner.LABEL_RUN_KEY, "cdap-app-runId")))
        .thenAnswer(i -> Stream.of(cluster));
    Mockito.when(cluster.getStatus())
        .thenReturn(ClusterStatus.RUNNING);
    Assert.assertEquals(cluster, provisioner.createCluster(context));

    //B. With preallocated cluster in "bad" state new allocation should happen.
    Mockito.when(cluster.getStatus())
        .thenReturn(ClusterStatus.FAILED);
    Mockito.when(cluster2.getName())
        .thenReturn("cluster2");
    DataprocConf conf = DataprocConf.create(
        provisioner.createContextProperties(context));
    ImmutableMap<String, String> reuseClusterFilter = ImmutableMap.of(
        AbstractDataprocProvisioner.LABEL_VERSON, "6_4",
        AbstractDataprocProvisioner.LABEL_REUSE_KEY, conf.getClusterReuseKey(),
        AbstractDataprocProvisioner.LABEL_PROFILE, "testProfile");

    Mockito.when(dataprocClient.getClusters(Mockito.eq(reuseClusterFilter), Mockito.any()))
        //B.1. When there is no good cluster found, a retry should happen
        .thenAnswer(i -> {
          //Ensure we call the predicate
          Predicate clusterPredicate = i.getArgument(1, Predicate.class);
          com.google.cloud.dataproc.v1.Cluster updatingCluster = com.google.cloud.dataproc.v1.Cluster.newBuilder()
              .setStatus(com.google.cloud.dataproc.v1.ClusterStatus.newBuilder()
                  .setState(State.UPDATING))
              .build();
          com.google.cloud.dataproc.v1.Cluster deletingCluster = com.google.cloud.dataproc.v1.Cluster.newBuilder()
              .setStatus(com.google.cloud.dataproc.v1.ClusterStatus.newBuilder()
                  .setState(State.DELETING))
              .build();
          Assert.assertFalse(clusterPredicate.test(updatingCluster));
          Assert.assertFalse(clusterPredicate.test(deletingCluster));
          return Stream.empty();
        })
        //B.2. Finally a reuse cluster found
        .thenAnswer(i -> Stream.of(cluster2));

    Assert.assertEquals(cluster2, provisioner.createCluster(context));

    Mockito.verify(dataprocClient)
        .updateClusterLabels("cluster2",
            Collections.singletonMap(AbstractDataprocProvisioner.LABEL_RUN_KEY, "cdap-app-runId"),
            Collections.singleton(AbstractDataprocProvisioner.LABEL_REUSE_UNTIL));
  }

  @Test
  public void testClusterMarkedForReuseOnDelete() throws Exception {
    context.addProperty("accountKey", "testKey");
    context.addProperty(DataprocConf.PROJECT_ID_KEY, "testProject");
    context.addProperty("region", "testRegion");
    context.addProperty("idleTTL", "5");
    context.addProperty(DataprocConf.SKIP_DELETE, "true");
    context.setErrorCategory(new ErrorCategory(ErrorCategoryEnum.DEPROVISIONING));
    DataprocConf conf = DataprocConf.create(
        provisioner.createContextProperties(context));
    Mockito.when(cluster.getName())
        .thenReturn("testClusterName");
    provisioner.doDeleteCluster(context, cluster, conf);

    Mockito.verify(dataprocClient)
        .updateClusterLabels(Mockito.eq("testClusterName"), addedLabelsCaptor.capture(),
            Mockito.eq(Collections.singleton(AbstractDataprocProvisioner.LABEL_RUN_KEY)));
    Assert.assertEquals(
        Collections.singleton(AbstractDataprocProvisioner.LABEL_REUSE_UNTIL),
        addedLabelsCaptor.getValue()
            .keySet());
  }

  @Test
  public void testVersionComparison() {
    DataprocImageVersion v0 = new DataprocImageVersion("0");
    DataprocImageVersion v1 = new DataprocImageVersion("1");
    DataprocImageVersion v1debian = new DataprocImageVersion("1-debian");
    DataprocImageVersion v1dot4 = new DataprocImageVersion("1.4");
    DataprocImageVersion v1dot4dot99 = new DataprocImageVersion("1.4.99");
    DataprocImageVersion v1dot5 = new DataprocImageVersion("1.5");
    DataprocImageVersion v1dot5debian = new DataprocImageVersion("1.5-debian");
    DataprocImageVersion v1dot5dot0 = new DataprocImageVersion("1.5.0");
    DataprocImageVersion v1dot5dot0debian = new DataprocImageVersion(
        "1.5.0-debian");
    DataprocImageVersion v1dot5dot0dot0 = new DataprocImageVersion("1.5.0.0");
    DataprocImageVersion v1dot5dot1 = new DataprocImageVersion("1.5.1");
    DataprocImageVersion v1dot6 = new DataprocImageVersion("1.6");
    DataprocImageVersion v2 = new DataprocImageVersion("2");
    DataprocImageVersion v2debian = new DataprocImageVersion("2-debian");
    DataprocImageVersion v2dot0 = new DataprocImageVersion("2.0");

    Assert.assertTrue(v1dot5.compareTo(v0) > 0);
    Assert.assertTrue(v1dot5.compareTo(v1) > 0);
    Assert.assertTrue(v1dot5.compareTo(v1debian) > 0);
    Assert.assertTrue(v1dot5.compareTo(v1dot4) > 0);
    Assert.assertTrue(v1dot5.compareTo(v1dot4dot99) > 0);
    Assert.assertTrue(v1dot5.compareTo(v1dot5) == 0);
    Assert.assertTrue(v1dot5.compareTo(v1dot5debian) == 0);
    Assert.assertTrue(v1dot5.compareTo(v1dot5dot0) == 0);
    Assert.assertTrue(v1dot5.compareTo(v1dot5dot0debian) == 0);
    Assert.assertTrue(v1dot5.compareTo(v1dot5dot0dot0) == 0);
    Assert.assertTrue(v1dot5.compareTo(v1dot5dot1) < 0);
    Assert.assertTrue(v1dot5.compareTo(v1dot6) < 0);
    Assert.assertTrue(v1dot5.compareTo(v2) < 0);
    Assert.assertTrue(v1dot5.compareTo(v2debian) < 0);
    Assert.assertTrue(v1dot5.compareTo(v2dot0) < 0);
  }

  @Test(expected = IllegalArgumentException.class)
  public void testNullDataprocImageVersion() {
    DataprocImageVersion nullVersion = new DataprocImageVersion(null);
  }

  @Test(expected = IllegalArgumentException.class)
  public void testEmptyDataprocImageVersion() {
    DataprocImageVersion emptyVersion = new DataprocImageVersion("");
  }

  @Test(expected = IllegalArgumentException.class)
  public void testInvalidSegmentDataprocImageVersion() {
    DataprocImageVersion emptyVersion = new DataprocImageVersion("1.2.3..6");
  }

  @Test(expected = IllegalArgumentException.class)
  public void testInvalidDataprocImageVersion() {
    DataprocImageVersion emptyVersion = new DataprocImageVersion("abcd");
  }

  @Test
  public void testCommonDataprocLabels() {
    context.addProperty("labels", "user|cdap;program|data-pipeline");
    Map<String, String> labels = provisioner.getCommonDataprocLabels(context);
    // Check that the value from system context has overwritten the provisioner context.
    Assert.assertEquals("system", labels.get("user"));
    // Check that the CDAP version is set by system context.
    Assert.assertEquals("6_4", labels.get("cdap-version"));
    // Check that the non-overwritten value from user context is present.
    Assert.assertEquals("data-pipeline", labels.get("program"));
    // Check that the unique value from system context is present.
    Assert.assertEquals("value", labels.get("system-prop"));
  }

  @Test
  public void testCommonDataprocLabelsInvalidLabel() {
    context.addProperty("labels", "email|user@cdapio");
    Map<String, String> labels = provisioner.getCommonDataprocLabels(context);
    Assert.assertNull(labels.get("email"));
  }
}
